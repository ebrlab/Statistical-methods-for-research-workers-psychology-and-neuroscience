{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import analysis packages\n",
    "%matplotlib inline\n",
    "import stan as ps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML as Center\n",
    "\n",
    "Center(\"\"\" <style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style> \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian estimation equivalent of the one-sample t-test\n",
    "\n",
    "This notebook follows nicely on from the Bayesian equivalent of the classical Z-test and provides an example equivalent of a Bayesian one sample t-test. However, the analyis below is conducted on a real dataset and demonstrates for a simple case the realities of fully probabalistic statistical modelling and dealing with uncertainty under the Bayesian framework.\n",
    "\n",
    "\n",
    "## The classic one sample t-test\n",
    "\n",
    "Before going any further, a brief review of the one sample t-test is given. The one-sample t-test calculates a t-statistic from calculating the difference between the sample mean and theorised known null hypothesis population mean and then finding the quotient of this difference over the estimated standard error from the sample data.\n",
    "\n",
    "\n",
    "$$ \\large t = \\frac{\\bar{X}-\\mu_0}{\\frac{S}{\\sqrt{N}}}$$ \n",
    "\n",
    "where\n",
    "\n",
    "$\\mu_0 = $ is a theorised constant for the population mean\n",
    "\n",
    "$\\bar{X} = $ sample mean\n",
    "\n",
    "$N = $ Sample size\n",
    "\n",
    "$S = $ sample standard deviation\n",
    "\n",
    "$\\frac{S}{\\sqrt{N}} = $ estimated standard error\n",
    "\n",
    "\n",
    "This t statistic is then used to calulate a p-value under a null sampling distribution with a certain degrees of freedom which is determiend by the smaple size ($df = n - 1$). If this p-value is $\\leq$ to the ùõº level pre-determined before the analysis under the Null hypothesis significance test the resuts is determined to be statistically signicant and the null hypothesis is rejected.\n",
    "\n",
    "$$ \\large H_0:\\bar{X} = \\mu_0$$\n",
    "$$ \\large H_1:\\bar{X}\\neq \\mu_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference\n",
    "<font size = \"3\"> Following on from that quick description of the classic one sample t-test above its important to keep in mind that Bayesian inference is all derived from the application of Bayes rule $P(\\theta \\mid y) = \\large \\frac{P(y \\mid \\theta) \\, P(\\theta)}{P(y)}$ and as such while the following description of the Bayesian model is an equivalent to the one sample t-test, it is fundamentally different, because it uses fully probabilistic modelling and the infernce is not based on sampling distributions.</font>\n",
    "    \n",
    "<font size = \"1\"> For a fuller description see the Practicing Bayesian statistics markdown file within the Github repository.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps of Bayesian data analysis\n",
    "\n",
    "<font size = \"3\"> Kruscke (2015) offers a step by step formulation for how to conduct a Bayesian analysis:\n",
    "\n",
    "1. Identify the relevant data for the question under investigation.\n",
    "\n",
    "2. Define the descriptive (mathematical) model for the data.\n",
    "\n",
    "3. Specify the Priors for the model. If scientific research publication is the goal the priors will need to be accepted by a skeptical audience. This should be achievable using prior predictive checks to ascertain if the priors are reasonable.\n",
    "\n",
    "4. Using Bayes rule estimate the posterior for the parameters of the model using the likelihood and priors. Then use the posterior for conducting your inferences.\n",
    "\n",
    "5. Conduct model checks. i.e. Posterior predcitive checks.</font> \n",
    "\n",
    "<font size = \"1\">This notebook will follow this approach generally.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Step 1 - Identify the relevant data for the question under investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study/data description\n",
    "\n",
    "The data analysed here were produced and analysed originally by Mehr, S. A., Song. L. A., & Spelke, E. S. (2016). For 5-month-old infants, melodies are social. Psychological Science, 27, 486-501. The data analysed below were downloaded from https://sites.google.com/view/openstatslab/home and stored in the same Github repository where these notebooks are stored.\n",
    "\n",
    "The research focused on the seemingly universal act of parents singing to their children and that this singing appears to focus their childs attention towards the parent that is singing. Mehr et al. (2016), aim was to study the function of this singing between mother and child. in particular Mehr et al. hypothesised that these specific melodies convey social information to infants.\n",
    "\n",
    "The resarchers argued that social groups share melodies and that different melodies exist between social groups. These shared melodies might signal to infants that novel individuals who sing these songs are from the same social group. Therefore, if a novel person sings a familiar melody this may signal to the infant group membership of this novel individual.\n",
    "\n",
    "All together, 32 parent and infant pairs were recuited with the aim of testing the hypothesis that melodies signal group membership to infants. The methodology they used to test this hypothesis involved taking each infant and mother pair in and  teaching them a new lullaby during theit first visit to the lab. The parents were then asked to sing the new lullaby to the infant every day for a 1-2 weeks before the return to the lab for the experimental session. \n",
    "\n",
    "The Experimental session consisted of showing the infants side by side videos of two unfamilliar individuals, on seperate screens. As with many infant studies the infant gaze is the standard behavioural measure for assessing infants attention. During the initial period of the experiment the two unfamiliar faces were presented with them both smiling at the infant silently. This allowed expert raters to produce baseline gaze proportions. This was followed by the each of the unique indivduals singing their lullaby (either the one the mother had been taught or one that had the sme rhythm, and lyrics but the melody was different). Folwwing this singing of the luublabies the infants were then left to view the two individuals when silent and smiling, to record another gaze proportion score.\n",
    "\n",
    "Of particular concern for this notebook is the Baseline proprtion scores, which were analysed in the orignal study using a one sample t-test with its goal to test if there was any bias in the infants basline gaze proportions as a group by comparing  the $\\bar{x}$ with a hypothsied proportion of $\\mu = .5$, to test for any statistical differnce. Mehr et al. did noto find a statistically significant differecne at the $\\alpha \\leq .05$ and proceeded to claim that this showed tere was no bias in baseline proprotion for either of the indivudals presented on the two screens. There are statistical reasoning faults of this approach due to the limits of the NHST that will not be discussed further here. However, for the interested parties Dienes (2014) explains the problem nicely and how Bayesian methods can offer a solution, and it will not be discussed further here, because the methods below offer a Bayesian estimation solution to the problem of determining any bias is baseline gaze probabilistically in the fashion of a Bayesian estiamtion equivalent of the one sample t-test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call github repository were the data is stored\n",
    "url = \"https://raw.githubusercontent.com/ebrlab/Statistical-methods-for-research-workers-bayes-for-psychologists-and-neuroscientists/master/Data/Mehr%20Song%20and%20Spelke%202016%20Experiment%201.csv\"\n",
    "\n",
    "# Import data .csv file into a pandas dataframe\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output data frame for evaluation\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data\n",
    "\n",
    "The analysis below is conducted on the experiment 1 data of Mehr et al. (2016), as such the data needs to be cleaned and reduced for clarity before condcuting any analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data for the specific experiment is in the first 32 rows of the dataframe\n",
    "red_df = df.iloc[0:32,]\n",
    "\n",
    "# unmark code below to output dataset for any checks that see fit (i.e. that extracting only experiment one)\n",
    "red_df.tail(5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualisation and exploratory data analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn theme for data visualisations.\n",
    "sns.set_style(\"white\");\n",
    "\n",
    "# PLotting a histogram and Kernel density estimate of the baseline gaze proportion scores\n",
    "sns.distplot(red_df[\"Baseline_Proportion_Gaze_to_Singer\"], fit = stats.norm, kde= False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just Eyeballing the data visualistaiton above might suggest that the data is not normally distributed. However, there are only 32 data points, so we have a high level of uncertainty around determining this. As such the use of a normal likelihood may still be appropriate and as the intial authors used a one sample t-test, which assumes normal distributed data, the analysis below will assume a normal likelihood for the Bayesain equivalent of the one-sample t-test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Define the descriptive statistical model \\begin{align*}\n",
    "y_i &\\sim Normal(\\mu, \\sigma) \n",
    "\\\\ \\mu &\\sim Normal(0.5, 0.2) \n",
    "\\\\ \\sigma &\\sim Exponential(0.1) \n",
    "\\end{align*} \n",
    "\n",
    "<font size = \"3\">The formulation for presenting statistical models here follows that used by McElreath (2020) for its intuitive nature. In plain english the model specifies that the dependent variable $y_i$ is distributed normally in terms of the Likelihood. The $\\mu$ and $\\sigma$ parameters are to be estimated. With the priors for the $\\mu$ parameter being $Normal(0.5, 0.2)$ and the $\\sigma$ being expoentially distributed $Exponential(0.01)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Specifying priors\n",
    "\n",
    "Following the description of the statistical model above the readers should rightly review and criticise the model freely, especialliy in asking why i selected these priors. The priors were selected using the prior predictive checks i ran in the code below to determine if my priors can generate data that falls reasonable on the dependent variable outcome space for the data generating process i am trying to model which in this case is expert rating of baseline gaze proportion scores of infants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior predictive checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise prior on mean parameter.\n",
    "x = np.arange(-0.5, 1.5, 0.001)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, stats.norm.pdf(x,loc=0.5, scale=0.2));\n",
    "plt.xlabel(\"prior on mu\");\n",
    "\n",
    "# Visualise prior on Standard deviation parameter.\n",
    "x = np.arange(-.01, .1, 0.001)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, stats.expon.pdf(x, scale = 0.01));\n",
    "plt.xlabel(\"prior on sigma\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating data based on priors\n",
    "\n",
    "Following the visualisation of the priors for the parameters of the model to \n",
    "check how they interact it is important to run prior predcitive check by \n",
    "simulating data based on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed to allow for the reproduciblity of notebook.\n",
    "np.random.seed(1)\n",
    "\n",
    "# Set the number of data point to sample\n",
    "n = 1000\n",
    "#Specify prior values for mu\n",
    "mu_loc = 0.5\n",
    "mu_scale =.2\n",
    "\n",
    "#Specify prior values for sigma\n",
    "sd_scale = .1\n",
    "\n",
    "\n",
    "# Simulate data from the priors for the mean and SD for the normal model specified above.\n",
    "sample_mu = np.random.normal(loc= mu_loc, scale = mu_scale, size = n )\n",
    "sample_sigma = np.random.exponential(scale = sd_scale, size = n )\n",
    "prior_PC = np.random.normal(loc = sample_mu, scale = sample_sigma, size = n)\n",
    "\n",
    "# Plot the simulated data\n",
    "sns.distplot(prior_PC, hist = False);\n",
    "\n",
    "# Plot vertical line of the 2 standard deviatons either side of the simulated data.\n",
    "plt.vlines((np.mean(prior_PC) + 2 * np.std(prior_PC)), ymin = 0, ymax = 1.8);\n",
    "plt.vlines((np.mean(prior_PC) - 2 * np.std(prior_PC)), ymin = 0, ymax = 1.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior predictive check show that generate reasonable data for baseline child gaze rated by expert rates on a proprotion scale between 0 and 1 follwing a normal model. Rember it doesnt matter if it looks like the data this is all in reltion to prior so everything is done in terms of assumptions of the unobserved data generating process (which in this case we assume is can be described by a normal pdf) not the data we actually observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Use Bayes rule\n",
    "The software of choice to conduct Bayesian inference on the data here is Stan (Carpenter et al., 2017) and the model is specified below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stan model of Bayesian One sample t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stan model to replciate the frequentist one sample t-test.\n",
    "\n",
    "One_sample_t_test_model = \"\"\"\n",
    "\n",
    "data{\n",
    "\n",
    "int<lower = 1> N;\n",
    "vector[N] y;\n",
    "\n",
    "}\n",
    "\n",
    "parameters{\n",
    "\n",
    "// Model paramaters to be estimated\n",
    "\n",
    " real<lower=0, upper=1> mu;// Bounded between 0 and 1 because as proportion of gaze scores cannot exceed those bounds\n",
    " real<lower=0> sigma; //Standard deviation bounded at 0\n",
    " \n",
    "}\n",
    "\n",
    "model{\n",
    "\n",
    "//priors\n",
    "// Informed prior based on the use of a normal likelihood to estimate mean and standard deviation parameters\n",
    "// of proportion score that ranges from 0 to 1\n",
    "\n",
    "mu ~ normal(0.5, 0.2);\n",
    "sigma ~ exponential(0.1);\n",
    "\n",
    "// Likliehood\n",
    "y ~ normal(mu, sigma);\n",
    "\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "\n",
    "//Generated a real value for difference between the MCMC sample of mu and 0.5.\n",
    "real diff = mu - 0.5;\n",
    "\n",
    "// Generating a quantity of a effect size similar to cohen D of a standarised diffence comparing\n",
    "// a reference mean (here is proportion of 0.5) as would be specified in a one sample t-test\n",
    "real Cohen_D = diff / sigma; \n",
    "  \n",
    "vector[N] yrep;\n",
    "  \n",
    "// Generate data for posterior samples\n",
    "  for (i in 1:N) {\n",
    "    yrep[i] = normal_rng(mu, sigma);\n",
    " }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StanModel function can be called and be passed the model string specified above to compile into C++ code.\n",
    "sm  = ps.StanModel(model_code = One_sample_t_test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate python dictionary to pass to Stan model to sample and run Bayesian One sample.\n",
    "data = {'N': len(red_df),\n",
    "        'y':  red_df[\"Baseline_Proportion_Gaze_to_Singer\"].values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and sample posterior for the model for the data, taking 2000 samples per 4 chains.\n",
    "fit = sm.sampling(data = data, iter = 2000, chains = 4, seed = 302675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of python pritn stament it is easier to extract the resut sint a panda data frame for data expression\n",
    "summary = fit.summary()\n",
    "fit_df = pd.DataFrame(summary['summary'], \n",
    "                  columns = summary['summary_colnames'], \n",
    "                  index = summary['summary_rownames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(fit[\"yrep\"].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post model fit-visualisations - Bayesian one sample Z-test\n",
    "\n",
    "## Posterior distributions plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using arviz built in Bayesian anlysis plot_posterior funcion to display the posterior probabilty distributions\n",
    "# for the fitted model parameter estimates.\n",
    "az.plot_posterior(fit, var_names=(\"mu\", \"sigma\"));\n",
    "az.plot_posterior(fit, var_names=(\"diff\", \"Cohen_D\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the arviz package the autocorrelation of the 4 MCMC chains can be plotted.\n",
    "az.plot_autocorr(fit, var_names=(\"mu\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autocorrelation plots do not show any serious autocorrelation problems, as the values quickly decrease to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC traceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(fit,var_names=(\"mu\", \"sigma\", \"diff\", \"Cohen_D\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traceplot show good mixing of chains and show a hairy catepillar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Posterior predictive checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrep_df  = pd.DataFrame(fit['yrep']).T.iloc[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert pystan fit object to IO for Arviz functions.\n",
    "data = az.from_pystan(\n",
    "                posterior=fit,\n",
    "                posterior_predictive='yrep',\n",
    "                observed_data=[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior simulated data sets for posterior predictive check\n",
    "az.plot_ppc(data, data_pairs = {\"y\" : \"yrep\"}, num_pp_samples= 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior predictive check shows that the simulated data sets condtioning the model on the data vary from  the original data sets. This could mean many things; the first being that an alternative better fitting model may need to be fit or   secondly that the small amount of data from a highly variable measure of infant gaze which has to go through a noisy expert rater) is presenting a simple reality that our model and its resulting inferences are more uncertain. Of course, this is not neccesarily a problem it just needs to be communicated to and appreciated by the audience that is viewing the results of the analysis, whom may even recommend improvements for the model and the modellers choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting the results of the Bayesian one sample t-test equivalent\n",
    "\n",
    "<font size = \"3\">As Kruscke correctly points out there is not standard formula or presentation method for results like the APA guide for reporting frequentist analyses using the Bayesian framework. It is likely there never will be, because as McElreath (2020) explains, Bayesian data analysis is more like a engineering approach to the problem and the resulting model that is fit will be analysis specific. In addition, as Gabry et al, (2019) have argued visualisations maybe even more key, so all the visualtions above would have to be included with any write up. Anyway, the write up below generally follows the advice of Kruscke (2015) chapter 25. In any application though it comes down to the problem to be described and the audience that needs to be convinced. </p><br/>\n",
    "\n",
    "<h2>Write up</h2><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., ... & Riddell, A. (2017). Stan: a probabilistic programming language. Grantee Submission, 76(1), 1-32.\n",
    "\n",
    "Dienes, Z. (2014). Using Bayes to get the most out of non-significant results. Frontiers in psychology, 5, 781.\n",
    "\n",
    "Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., & Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2), 389-402.\n",
    "    \n",
    "Kruschke, J. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS and Stan. Oxford, England: Academic Press.    \n",
    "    \n",
    "McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan.Boca Raton: CRC Press.\n",
    "\n",
    "Mehr, S. A., Song. L. A., & Spelke, E. S. (2016). For 5-month-old infants, melodies are social. Psychological Science, 27, 486-501."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stan",
   "language": "python",
   "name": "stan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "319px",
    "width": "873px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "315.806px"
   },
   "toc_section_display": true,
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
